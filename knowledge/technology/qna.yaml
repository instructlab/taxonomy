created_by: barny
version: 3
domain: large-language-model
document_outline: Knowledge contribution about the IBM Granite Model
seed_examples:
  - context: >-
      IBM Granite is a series of decoder-only AI foundation models created by
      IBM.[3] It was announced on September 7, 2023,[4][5] and an initial paper
      was published 4 days later.[6] Initially intended for use in the IBM's
      cloud-based data and generative AI platform Watsonx along with other
      models,[7] IBM opened the source code of some code models.[8][9] Granite
      models are trained on datasets curated from Internet, academic
      publishings, code datasets, legal and finance documents
    questions_and_answers:
      - question: What is IBM Granite?
        answer: >-
          IBM Granite is a series of decoder-only AI foundation models created
          by IBM.
      - question: When as was IBM Granite announced?
        answer: September 7, 2023
      - question: What's a series of IBM decoder-only AI foundation models?
        answer: IBM Granite
  - context: >-
      Granite's first foundation models were Granite.13b.instruct and
      Granite.13b.chat. The "13b" in their name comes from 13 billion, the
      amount of parameters they have as models, lesser than most of the larger
      models of the time. Later models vary from 3 to 34 billion parameters.
    questions_and_answers:
      - question: what were the first foundation models in IBm Granite?
        answer: Granite.13b.instruct and Granite.13b.chat
      - question: >-
          How many model parameters do Granite.13b.instruct and Granite.13b.chat
          have?
        answer: 13 billion
      - question: Do all Granite models have the same number of model parameters?
        answer: Models vary from 3 to 34 billion parameters.
  - context: >
      On May 6, 2024, IBM released the source code of four variations of Granite
      Code Models under Apache 2, an open source permissive license that allows
      completely free use, modification and sharing of the software, and put
      them on Hugging Face for public use.[14][15] According to IBM's own
      report, Granite 8b outperforms Llama 3 on several coding related tasks
      within similar range of parameters.
    questions_and_answers:
      - question: When did IBM first release the source code of Granite Code Models?
        answer: May 6, 2024
      - question: Does Granite outperform other LLMs?
        answer: >-
          Granite 8b outperforms Llama 3 on several coding related tasks within
          similar range of parameters
      - question: Where are the Granite models and code accessed?
        answer: IBM put them on Hugging Face for public use.
  - context: >-
      A foundation model is an AI model trained on broad data at scale such that
      it can be adapted to a wide range of downstream tasks
    questions_and_answers:
      - question: what is a foundation model?
        answer: 'A foundation model is an AI model trained on broad data '
      - question: What scale of data is a foundation model trained on?
        answer: >-
          the data is at scale such that it can be adapted to a wide range of
          downstream tasks
      - question: why are these tasks referred to as downstream?
        answer: >-
          "Downstream" means "derived from" or "based on"; downstream tasks are
          work that is done using the foundation model.
  - context: >-
      IBM opened the source code of some code models.[8][9] Granite models are
      trained on datasets curated from Internet, academic publishings, code
      datasets, legal and finance documents
    questions_and_answers:
      - question: Did IBM release the full source code for IBM Granite?
        answer: IBM released source code of four code models.
      - question: What datasets is Granite trained on?
        answer: >-
          As well as datasets curated from the internet,  Granite is trained on
          academic publications, code datasets, legal and finance documents
      - question: Are the Granite training datasets open source?
        answer: 'Yes'
document:
  repo: https://github.com/barny/taxonomy-knowledge-docs
  commit: 260ddda8c25526f200a5fe60e3898cf303e2430c
  patterns:
    - granite-20250123T172453388.md
