created_by: mormdan
version: 3
domain: large-language-model
document_outline: Knowledge contribution about the IBM Granite model.
seed_examples:
  - context: >-
      IBM Granite is a series of decoder-only AI foundation models created by
      IBM. It was announced on September 7,2023, and an initial paper was
      published 4 days later.
    questions_and_answers:
      - question: What is IBM Granite?
        answer: >-
          IBM Granite is a series of decoder-only AI foundation models created
          by IBM.
      - question: When was IBM Granite announced?
        answer: September 7,2023
      - question: What's series of IBM decoder-only AI foundation models?
        answer: IBM Granite
  - context: >-
      Initially intended for use in the IBM's cloud-based data and generative AI
      platform Watsonx along with other models, IBM opened the source code of
      some code models. Granite models are trained on datasets curated from
      Internet, academic publishings, code datasets, legal and finance
      documents.
    questions_and_answers:
      - question: How did the Granite models train?
        answer: >-
          Granite models are trained on datasets curated from Internet, academic
          publishings, code datasets, legal and finance documents
      - question: What was the initial intension usage for Granite model?
        answer: >-
          Initially intended for use in the IBM's cloud-based data and
          generative AI platform Watsonx along with other models
      - question: How did IBM enable Granite collaboration?
        answer: IBM opened the source code of some code models
  - context: >-
      A foundation model is an AI model trained on broad data at scale such that
      it can be adapted to a wide range of downstream tasks.

      Granite's first foundation models were Granite.13b.instruct and
      Granite.13b.chat. The "13b" in their name comes from 13 billion, the
      amount of parameters they have as models, lesser than most of the larger
      models of the time. Later models vary from 3 to 34 billion parameters.

      On May 6, 2024, IBM released the source code of four variations of Granite
      Code Models under Apache 2, an open source permissive license that allows
      completely free use, modification and sharing of the software, and put
      them on Hugging Face for public use. According to IBM's own report,
      Granite 8b outperforms Llama 3 on several coding related tasks within
      similar range of parameters.
    questions_and_answers:
      - question: what is a foundation model?
        answer: >-
          The foundation model is an AI model trained on broad data at scale
          such that it can be adapted to a wide range of downstream tasks.
      - question: what are the Granite first foundation models?
        answer: Granite.13b.instruct and Granite.13b.chat
      - question: what is the meaning of the 13b in the Granite first foundation models?
        answer: >-
          It comes from 13 billion, which is the number of parameters they have
          as models.
  - context: >-
      Granite's first foundation models were Granite.13b.instruct and
      Granite.13b.chat. The "13b" in their name comes from 13 billion, the
      amount of parameters they have as models, lesser than most of the larger
      models of the time. Later models vary from 3 to 34 billion parameters.

      On May 6, 2024, IBM released the source code of four variations of Granite
      Code Models under Apache 2, an open source permissive license that allows
      completely free use, modification and sharing of the software, and put
      them on Hugging Face for public use. According to IBM's own report,
      Granite 8b outperforms Llama 3 on several coding related tasks within
      similar range of parameters.
    questions_and_answers:
      - question: 'What is the number of parameters that in the Granite later models? '
        answer: Granite later models vary from 3 to 34 billion parameters.
      - question: >-
          When did IBM release the source code of four variations of Granite
          Code models?
        answer: May 6, 2024
      - question: >-
          Under which apache version did IBM released the source code of Granite
          code models?
        answer: Apache 2
  - context: >-
      On May 6, 2024, IBM released the source code of four variations of Granite
      Code Models under Apache 2, an open source permissive license that allows
      completely free use, modification and sharing of the software, and put
      them on Hugging Face for public use. According to IBM's own report,
      Granite 8b outperforms Llama 3 on several coding related tasks within
      similar range of parameters.
    questions_and_answers:
      - question: Where does the Granite open source models put for public use?
        answer: on Hugging Face.
      - question: What was the Granite source code model released on May 6,2024 ?
        answer: >-
          An open source permissive license that allows completely free use,
          modification and sharing of the software, and put them on Hugging Face
          for public use.
      - question: >-
          Is the Granite 8b outperforms Llama 3 on several coding related tasks
          within similar range of parameters ?
        answer: >-
          Yes, According to IBM's own report, Granite 8b outperforms Llama 3 on
          several coding related tasks within similar range of parameters.
document:
  repo: https://github.com/mormdan/taxonomy-knowledge-docs
  commit: b98786c677ce06cc5ae040606239a0dd251f8fbc
  patterns:
    - IBM-Granite-20241201T110029896.md
