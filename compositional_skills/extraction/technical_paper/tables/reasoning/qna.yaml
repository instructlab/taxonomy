created_by: IBM
seed_examples:
- answer: "The Mushroom dataset resulted in the highest F1 score (w/ post processing).\
    \ The range of F1 scores for the Mushroom dataset ranged from 0.68 to 0.93. The\
    \ Adult dataset ranged from 0.33 to 0.61. The shopper dataset ranged from 0.3\
    \ to 0.48 and the Bank dataset ranged from 0.23 to 0.46. Additionally, the F1\
    \ scores for the real data were the highest for the Mushroom dataset. \n"
  document: "Table 1: We compare synthetic data generated without and with our post-processing\
    \ technique, all under the same privacy budget \u03F5 =2. We demonstrate utility\
    \ improvement (higher is better, positive numbers imply improvement) and F1 score\
    \ for downstream models trained on synthetic data and tested on real data. For\
    \ reference, when training the same downstream model using real data, the F1 scores\
    \ are: (Adult \u21AA 0 \xB1 61), (Mushroom \u21AA 0 \xB1 95), (Shopper \u21AA\
    \ 0 \xB1 54), and (Bank \u21AA 0 \xB1 47). Additionally, we measure the average\
    \ Jensen-Shannon (JS) distance between the marginal distributions of synthetic\
    \ and real data (0: identical distribution; 1: totally different distributions)\
    \ and the average inverse KL-divergence (0: totally different distributions; 1:\
    \ identical distribution). As shown, our technique consistently improves the utility\
    \ of the synthetic data across all datasets and all DP mechanisms without degrading\
    \ the performance of downstream models or statistical metrics.\n\n|\_\_\_\_\_\
    |\_\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_| F1 score\_\_\_\_| F1 score\_\_\_\_\_| JS distance\
    \ (marginal)\_\_| JS distance (marginal)\_\_| Inverse KL (marginal)\_\_| Inverse\
    \ KL (marginal)\_\_|\n|----------|--------------|-----------------|----------------|-------------------|--------------------------|--------------------------|-------------------------|-------------------------|\n\
    | Dataset\_| DP Mechanism | Utility Improv. | w/o post-proc. | w/ post-proc.\_\
    \_\_| w/o post-proc.\_\_\_\_\_\_| w/ post-proc.\_\_\_\_\_\_| w/o post-proc.\_\_\
    \_\_\_| w/ post-proc.\_\_\_\_\_\_|\n| Adult\_\_| AIM\_\_\_\_\_| 0.13 \xB7 0 \xB1\
    \ 03\_\_| 0.61\_\_\_\_\_\_| 0.61\xB7 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n\
    |\_\_\_\_\_| MST\_\_\_\_\_| 0.22 \xB7 0 \xB1 02\_\_| 0.55\_\_\_\_\_\_| 0.56\xB7\
    \ 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\
    \_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.81\
    \ \xB7 0 \xB1 09\_\_| 0.22\_\_\_\_\_\_| 0.33\xB7 0 \xB1 02 | 0.07\_\_\_\_\_\_\_\
    \_\_\_\_| 0.03\xB7 0 \xB1 02\_\_\_\_| 0.85\_\_\_\_\_\_\_\_\_\_| 0.91\xB7 0 \xB1\
    \ 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.6 \xB7 0 \xB1 04\_\_| 0.37\_\_\_\_\
    \_\_| 0.5\xB7 0 \xB1 03\_| 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.72\_\_\_\_\_\_\_\_\_\_| 0.87\xB7 0 \xB1 0\_\_\_\_|\n| Mushroom | AIM\_\
    \_\_\_\_| 0.12 \xB7 0 \xB1 0\_\_| 0.93\_\_\_\_\_\_| 0.93\xB7 0 \xB1 0\_| 0.01\_\
    \_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.58 \xB7 0 \xB1 0\_\_| 0.83\_\
    \_\_\_\_\_| 0.83\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\
    \_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_\
    | DPCTGAN\_\_\_| 0.69 \xB7 0 \xB1 04\_\_| 0.47\_\_\_\_\_\_| 0.68\xB7 0 \xB1 01\
    \ | 0.05\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\
    \_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.83 \xB7 0 \xB1\
    \ 05\_\_| 0.6\_\_\_\_\_\_| 0.86\xB7 0 \xB1 04 | 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7\
    \ 0 \xB1 0\_\_\_\_\_| 0.87\_\_\_\_\_\_\_\_\_\_| 0.95\xB7 0 \xB1 0\_\_\_\_|\n|\
    \ Shopper\_| AIM\_\_\_\_\_| 0.1 \xB7 0 \xB1 02\_\_| 0.48\_\_\_\_\_\_| 0.48\xB7\
    \ 0 \xB1 02 | 0.02\_\_\_\_\_\_\_\_\_\_\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0.84\_\_\
    \_\_\_\_\_\_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.5\
    \ \xB7 0 \xB1 02\_\_| 0.42\_\_\_\_\_\_| 0.47\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\
    \_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\
    \_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.36 \xB7 0 \xB1 05\_\_| 0.27\_\_\_\_\
    \_\_| 0.3\xB7 0 \xB1 02\_| 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.74\_\_\_\_\_\_\_\_\_\_| 0.85\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.11 \xB7 0 \xB1 04\_\_| 0.25\_\_\_\_\_\_| 0.31\xB7 0 \xB1 05 | 0.04\_\_\_\
    \_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.8\_\_\_\_\_\_\_\_\_\_\_| 0.89\xB7\
    \ 0 \xB1 0\_\_\_\_|\n| Bank\_\_\_| AIM\_\_\_\_\_| 0.18 \xB7 0 \xB1 01\_\_| 0.45\_\
    \_\_\_\_\_| 0.46\xB7 0 \xB1 01 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7 0 \xB1 0\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\
    \_\_\_\_| 0.32 \xB7 0 \xB1 02\_\_| 0.43\_\_\_\_\_\_| 0.44\xB7 0 \xB1 02 | 0.02\_\
    \_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.98\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.2 \xB7 0 \xB1 02\_\_| 0.22\_\
    \_\_\_\_\_| 0.24\xB7 0 \xB1 07 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.71\_\_\_\_\_\_\_\_\_\_| 0.88\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.25 \xB7 0 \xB1 03\_\_| 0.2\_\_\_\_\_\_| 0.23\xB7 0 \xB1 05 | 0.03\_\_\_\_\
    \_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7\
    \ 0 \xB1 0\_\_\_\_\_|\n\nTable 2: Experimental results on the home-credit dataset.\
    \ We compare synthetic data produced by GEM with \u03F5 \u2208 \xB6 2 \u21AA 4\
    \ \u2666 against the post-processed synthetic data that are generated by applying\
    \ our post-processing technique with \u03F5$_{post =1 to synthetic data generated\
    \ from GEM with \u03F5 \u2208 \xB6 1 \u21AA 3 \u2666. For reference, the downstream\
    \ model trained and tested on real data has an F1 score of 0 \xB1 24.\n\n| DP\
    \ Mechanism\_\_| Utility Improv.\_\_|\_\_F1 score w/o post-proc. | F1 score w/\
    \ post-proc.\_\_|\_\_JS distance w/o post-proc. | JS distance w/ post-proc.\_\_\
    |\_\_Inverse KL w/o post-proc. | Inverse KL w/ post-proc.\_\_|\n|----------------|-------------------|---------------------------|--------------------------|------------------------------|-----------------------------|-----------------------------|----------------------------|\n\
    | GEM \u03F5 =2\_\_\_\_| 0.57 \xB7 0 \xB1 03\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.19\
    \ | 0.19\xB7 0 \xB1 02\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.93 | 0.97\xB7 0 \xB1 01\_\_\_\_\_|\n\
    | GEM \u03F5 =4\_\_\_\_| 0.68 \xB7 0 \xB1 01\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.21\
    \ | 0.21\xB7 0 \xB1 01\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.95 | 0.99\xB7 0 \xB1 01\_\_\_\_\_|\n\
    \nResult. We present the experimental results on the UCI datasets in Table 1 (and\
    \ Table 3 in Appendix C with a higher privacy budget). We report utility improvement,\
    \ F1 score, and statistical measures. As shown, our post-processing technique\
    \ consistently enhances the utility of synthetic data on selected metrics across\
    \ all benchmark datasets and all privacy mechanisms. Moreover, the utility improvements\
    \ are achieved without degrading the performance of downstream models or statistical\
    \ properties of synthetic data. In other words, our post-processing procedure\
    \ ensures that the logistic regression classifier trained on synthetic data can\
    \ achieve comparable or even higher performance on real test data, while simultaneously\
    \ reducing or maintaining the statistical divergences between synthetic and real\
    \ data.\n\nWe present the experimental results on the home-credit dataset in Table\
    \ 2 and illustrate the misalignment of the correlation matrix with and without\
    \ applying our post-processing procedure in Figure 1. The results demonstrate\
    \ that our algorithm consistently reduces the overall correlation misalignment.\
    \ Additionally, our procedure, which includes computing the utility measures from\
    \ real data, denoising the noisy answers, and computing optimal resampling weights,\
    \ only takes around 4 mins on 1x NVIDIA GeForce RTX 3090 GPU.\n"
  question: 'Which dataset resulted in the highest performance according to the F1
    score (w/post processing)?

    '
- answer: 'The highest average utility was for the DPCTGAN approach with 0.515 utility
    improvement. The average utility improvement was 0.1325 for the AIM approach.
    The average improvement for MST was 0.405. The average for DPCTGAN was 0.515.The
    PATECTGAN mechanism was 0.4475.

    '
  document: "Table 1: We compare synthetic data generated without and with our post-processing\
    \ technique, all under the same privacy budget \u03F5 =2. We demonstrate utility\
    \ improvement (higher is better, positive numbers imply improvement) and F1 score\
    \ for downstream models trained on synthetic data and tested on real data. For\
    \ reference, when training the same downstream model using real data, the F1 scores\
    \ are: (Adult \u21AA 0 \xB1 61), (Mushroom \u21AA 0 \xB1 95), (Shopper \u21AA\
    \ 0 \xB1 54), and (Bank \u21AA 0 \xB1 47). Additionally, we measure the average\
    \ Jensen-Shannon (JS) distance between the marginal distributions of synthetic\
    \ and real data (0: identical distribution; 1: totally different distributions)\
    \ and the average inverse KL-divergence (0: totally different distributions; 1:\
    \ identical distribution). As shown, our technique consistently improves the utility\
    \ of the synthetic data across all datasets and all DP mechanisms without degrading\
    \ the performance of downstream models or statistical metrics.\n\n|\_\_\_\_\_\
    |\_\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_| F1 score\_\_\_\_| F1 score\_\_\_\_\_| JS distance\
    \ (marginal)\_\_| JS distance (marginal)\_\_| Inverse KL (marginal)\_\_| Inverse\
    \ KL (marginal)\_\_|\n|----------|--------------|-----------------|----------------|-------------------|--------------------------|--------------------------|-------------------------|-------------------------|\n\
    | Dataset\_| DP Mechanism | Utility Improv. | w/o post-proc. | w/ post-proc.\_\
    \_\_| w/o post-proc.\_\_\_\_\_\_| w/ post-proc.\_\_\_\_\_\_| w/o post-proc.\_\_\
    \_\_\_| w/ post-proc.\_\_\_\_\_\_|\n| Adult\_\_| AIM\_\_\_\_\_| 0.13 \xB7 0 \xB1\
    \ 03\_\_| 0.61\_\_\_\_\_\_| 0.61\xB7 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n\
    |\_\_\_\_\_| MST\_\_\_\_\_| 0.22 \xB7 0 \xB1 02\_\_| 0.55\_\_\_\_\_\_| 0.56\xB7\
    \ 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\
    \_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.81\
    \ \xB7 0 \xB1 09\_\_| 0.22\_\_\_\_\_\_| 0.33\xB7 0 \xB1 02 | 0.07\_\_\_\_\_\_\_\
    \_\_\_\_| 0.03\xB7 0 \xB1 02\_\_\_\_| 0.85\_\_\_\_\_\_\_\_\_\_| 0.91\xB7 0 \xB1\
    \ 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.6 \xB7 0 \xB1 04\_\_| 0.37\_\_\_\_\
    \_\_| 0.5\xB7 0 \xB1 03\_| 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.72\_\_\_\_\_\_\_\_\_\_| 0.87\xB7 0 \xB1 0\_\_\_\_|\n| Mushroom | AIM\_\
    \_\_\_\_| 0.12 \xB7 0 \xB1 0\_\_| 0.93\_\_\_\_\_\_| 0.93\xB7 0 \xB1 0\_| 0.01\_\
    \_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.58 \xB7 0 \xB1 0\_\_| 0.83\_\
    \_\_\_\_\_| 0.83\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\
    \_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_\
    | DPCTGAN\_\_\_| 0.69 \xB7 0 \xB1 04\_\_| 0.47\_\_\_\_\_\_| 0.68\xB7 0 \xB1 01\
    \ | 0.05\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\
    \_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.83 \xB7 0 \xB1\
    \ 05\_\_| 0.6\_\_\_\_\_\_| 0.86\xB7 0 \xB1 04 | 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7\
    \ 0 \xB1 0\_\_\_\_\_| 0.87\_\_\_\_\_\_\_\_\_\_| 0.95\xB7 0 \xB1 0\_\_\_\_|\n|\
    \ Shopper\_| AIM\_\_\_\_\_| 0.1 \xB7 0 \xB1 02\_\_| 0.48\_\_\_\_\_\_| 0.48\xB7\
    \ 0 \xB1 02 | 0.02\_\_\_\_\_\_\_\_\_\_\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0.84\_\_\
    \_\_\_\_\_\_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.5\
    \ \xB7 0 \xB1 02\_\_| 0.42\_\_\_\_\_\_| 0.47\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\
    \_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\
    \_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.36 \xB7 0 \xB1 05\_\_| 0.27\_\_\_\_\
    \_\_| 0.3\xB7 0 \xB1 02\_| 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.74\_\_\_\_\_\_\_\_\_\_| 0.85\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.11 \xB7 0 \xB1 04\_\_| 0.25\_\_\_\_\_\_| 0.31\xB7 0 \xB1 05 | 0.04\_\_\_\
    \_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.8\_\_\_\_\_\_\_\_\_\_\_| 0.89\xB7\
    \ 0 \xB1 0\_\_\_\_|\n| Bank\_\_\_| AIM\_\_\_\_\_| 0.18 \xB7 0 \xB1 01\_\_| 0.45\_\
    \_\_\_\_\_| 0.46\xB7 0 \xB1 01 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7 0 \xB1 0\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\
    \_\_\_\_| 0.32 \xB7 0 \xB1 02\_\_| 0.43\_\_\_\_\_\_| 0.44\xB7 0 \xB1 02 | 0.02\_\
    \_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.98\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.2 \xB7 0 \xB1 02\_\_| 0.22\_\
    \_\_\_\_\_| 0.24\xB7 0 \xB1 07 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.71\_\_\_\_\_\_\_\_\_\_| 0.88\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.25 \xB7 0 \xB1 03\_\_| 0.2\_\_\_\_\_\_| 0.23\xB7 0 \xB1 05 | 0.03\_\_\_\_\
    \_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7\
    \ 0 \xB1 0\_\_\_\_\_|\n\nTable 2: Experimental results on the home-credit dataset.\
    \ We compare synthetic data produced by GEM with \u03F5 \u2208 \xB6 2 \u21AA 4\
    \ \u2666 against the post-processed synthetic data that are generated by applying\
    \ our post-processing technique with \u03F5$_{post =1 to synthetic data generated\
    \ from GEM with \u03F5 \u2208 \xB6 1 \u21AA 3 \u2666. For reference, the downstream\
    \ model trained and tested on real data has an F1 score of 0 \xB1 24.\n\n| DP\
    \ Mechanism\_\_| Utility Improv.\_\_|\_\_F1 score w/o post-proc. | F1 score w/\
    \ post-proc.\_\_|\_\_JS distance w/o post-proc. | JS distance w/ post-proc.\_\_\
    |\_\_Inverse KL w/o post-proc. | Inverse KL w/ post-proc.\_\_|\n|----------------|-------------------|---------------------------|--------------------------|------------------------------|-----------------------------|-----------------------------|----------------------------|\n\
    | GEM \u03F5 =2\_\_\_\_| 0.57 \xB7 0 \xB1 03\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.19\
    \ | 0.19\xB7 0 \xB1 02\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.93 | 0.97\xB7 0 \xB1 01\_\_\_\_\_|\n\
    | GEM \u03F5 =4\_\_\_\_| 0.68 \xB7 0 \xB1 01\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.21\
    \ | 0.21\xB7 0 \xB1 01\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.95 | 0.99\xB7 0 \xB1 01\_\_\_\_\_|\n\
    \nResult. We present the experimental results on the UCI datasets in Table 1 (and\
    \ Table 3 in Appendix C with a higher privacy budget). We report utility improvement,\
    \ F1 score, and statistical measures. As shown, our post-processing technique\
    \ consistently enhances the utility of synthetic data on selected metrics across\
    \ all benchmark datasets and all privacy mechanisms. Moreover, the utility improvements\
    \ are achieved without degrading the performance of downstream models or statistical\
    \ properties of synthetic data. In other words, our post-processing procedure\
    \ ensures that the logistic regression classifier trained on synthetic data can\
    \ achieve comparable or even higher performance on real test data, while simultaneously\
    \ reducing or maintaining the statistical divergences between synthetic and real\
    \ data.\n\nWe present the experimental results on the home-credit dataset in Table\
    \ 2 and illustrate the misalignment of the correlation matrix with and without\
    \ applying our post-processing procedure in Figure 1. The results demonstrate\
    \ that our algorithm consistently reduces the overall correlation misalignment.\
    \ Additionally, our procedure, which includes computing the utility measures from\
    \ real data, denoising the noisy answers, and computing optimal resampling weights,\
    \ only takes around 4 mins on 1x NVIDIA GeForce RTX 3090 GPU.\n"
  question: Which DP Mechanism resulted in the highest average utility improvement
    across all datasets.
- answer: 'The Utility improvement is higher. We first took the difference between
    F1 scores subtracting the F1 for w/post processing minus F1 score with post processing.
    This difference ranged from 0 to 0.26 with an average difference of 0.058125.
    The difference in utility improvement ragned from 0.1 to 0.81 with an average
    of 0.375.

    '
  document: "Table 1: We compare synthetic data generated without and with our post-processing\
    \ technique, all under the same privacy budget \u03F5 =2. We demonstrate utility\
    \ improvement (higher is better, positive numbers imply improvement) and F1 score\
    \ for downstream models trained on synthetic data and tested on real data. For\
    \ reference, when training the same downstream model using real data, the F1 scores\
    \ are: (Adult \u21AA 0 \xB1 61), (Mushroom \u21AA 0 \xB1 95), (Shopper \u21AA\
    \ 0 \xB1 54), and (Bank \u21AA 0 \xB1 47). Additionally, we measure the average\
    \ Jensen-Shannon (JS) distance between the marginal distributions of synthetic\
    \ and real data (0: identical distribution; 1: totally different distributions)\
    \ and the average inverse KL-divergence (0: totally different distributions; 1:\
    \ identical distribution). As shown, our technique consistently improves the utility\
    \ of the synthetic data across all datasets and all DP mechanisms without degrading\
    \ the performance of downstream models or statistical metrics.\n\n|\_\_\_\_\_\
    |\_\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_| F1 score\_\_\_\_| F1 score\_\_\_\_\_| JS distance\
    \ (marginal)\_\_| JS distance (marginal)\_\_| Inverse KL (marginal)\_\_| Inverse\
    \ KL (marginal)\_\_|\n|----------|--------------|-----------------|----------------|-------------------|--------------------------|--------------------------|-------------------------|-------------------------|\n\
    | Dataset\_| DP Mechanism | Utility Improv. | w/o post-proc. | w/ post-proc.\_\
    \_\_| w/o post-proc.\_\_\_\_\_\_| w/ post-proc.\_\_\_\_\_\_| w/o post-proc.\_\_\
    \_\_\_| w/ post-proc.\_\_\_\_\_\_|\n| Adult\_\_| AIM\_\_\_\_\_| 0.13 \xB7 0 \xB1\
    \ 03\_\_| 0.61\_\_\_\_\_\_| 0.61\xB7 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n\
    |\_\_\_\_\_| MST\_\_\_\_\_| 0.22 \xB7 0 \xB1 02\_\_| 0.55\_\_\_\_\_\_| 0.56\xB7\
    \ 0 \xB1 0\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\
    \_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.81\
    \ \xB7 0 \xB1 09\_\_| 0.22\_\_\_\_\_\_| 0.33\xB7 0 \xB1 02 | 0.07\_\_\_\_\_\_\_\
    \_\_\_\_| 0.03\xB7 0 \xB1 02\_\_\_\_| 0.85\_\_\_\_\_\_\_\_\_\_| 0.91\xB7 0 \xB1\
    \ 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.6 \xB7 0 \xB1 04\_\_| 0.37\_\_\_\_\
    \_\_| 0.5\xB7 0 \xB1 03\_| 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.72\_\_\_\_\_\_\_\_\_\_| 0.87\xB7 0 \xB1 0\_\_\_\_|\n| Mushroom | AIM\_\
    \_\_\_\_| 0.12 \xB7 0 \xB1 0\_\_| 0.93\_\_\_\_\_\_| 0.93\xB7 0 \xB1 0\_| 0.01\_\
    \_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.58 \xB7 0 \xB1 0\_\_| 0.83\_\
    \_\_\_\_\_| 0.83\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\_\_\_\_| 0\xB7 0 \xB1 0\_\_\
    \_\_\_\_| 1\_\_\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_\
    | DPCTGAN\_\_\_| 0.69 \xB7 0 \xB1 04\_\_| 0.47\_\_\_\_\_\_| 0.68\xB7 0 \xB1 01\
    \ | 0.05\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\
    \_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\_| 0.83 \xB7 0 \xB1\
    \ 05\_\_| 0.6\_\_\_\_\_\_| 0.86\xB7 0 \xB1 04 | 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7\
    \ 0 \xB1 0\_\_\_\_\_| 0.87\_\_\_\_\_\_\_\_\_\_| 0.95\xB7 0 \xB1 0\_\_\_\_|\n|\
    \ Shopper\_| AIM\_\_\_\_\_| 0.1 \xB7 0 \xB1 02\_\_| 0.48\_\_\_\_\_\_| 0.48\xB7\
    \ 0 \xB1 02 | 0.02\_\_\_\_\_\_\_\_\_\_\_| 0.01\_\_\_\_\_\_\_\_\_\_\_| 0.84\_\_\
    \_\_\_\_\_\_\_\_| 0.92\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| MST\_\_\_\_\_| 0.5\
    \ \xB7 0 \xB1 02\_\_| 0.42\_\_\_\_\_\_| 0.47\xB7 0 \xB1 02 | 0.01\_\_\_\_\_\_\_\
    \_\_\_\_| 0\xB7 0 \xB1 0\_\_\_\_\_\_| 0.99\_\_\_\_\_\_\_\_\_\_| 1\xB7 0 \xB1 0\_\
    \_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.36 \xB7 0 \xB1 05\_\_| 0.27\_\_\_\_\
    \_\_| 0.3\xB7 0 \xB1 02\_| 0.03\_\_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\
    \_\_| 0.74\_\_\_\_\_\_\_\_\_\_| 0.85\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.11 \xB7 0 \xB1 04\_\_| 0.25\_\_\_\_\_\_| 0.31\xB7 0 \xB1 05 | 0.04\_\_\_\
    \_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.8\_\_\_\_\_\_\_\_\_\_\_| 0.89\xB7\
    \ 0 \xB1 0\_\_\_\_|\n| Bank\_\_\_| AIM\_\_\_\_\_| 0.18 \xB7 0 \xB1 01\_\_| 0.45\_\
    \_\_\_\_\_| 0.46\xB7 0 \xB1 01 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7 0 \xB1 0\_\_\_\_\_|\n|\_\_\_\_\_| MST\_\
    \_\_\_\_| 0.32 \xB7 0 \xB1 02\_\_| 0.43\_\_\_\_\_\_| 0.44\xB7 0 \xB1 02 | 0.02\_\
    \_\_\_\_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.98\_\_\_\_\_\_\_\_\_\_| 1\xB7\
    \ 0 \xB1 0\_\_\_\_\_\_|\n|\_\_\_\_\_| DPCTGAN\_\_\_| 0.2 \xB7 0 \xB1 02\_\_| 0.22\_\
    \_\_\_\_\_| 0.24\xB7 0 \xB1 07 | 0.04\_\_\_\_\_\_\_\_\_\_\_| 0.02\xB7 0 \xB1 01\_\
    \_\_\_| 0.71\_\_\_\_\_\_\_\_\_\_| 0.88\xB7 0 \xB1 0\_\_\_\_|\n|\_\_\_\_\_| PATECTGAN\_\
    \_| 0.25 \xB7 0 \xB1 03\_\_| 0.2\_\_\_\_\_\_| 0.23\xB7 0 \xB1 05 | 0.03\_\_\_\_\
    \_\_\_\_\_\_\_| 0.01\xB7 0 \xB1 0\_\_\_\_\_| 0.83\_\_\_\_\_\_\_\_\_\_| 0.9\xB7\
    \ 0 \xB1 0\_\_\_\_\_|\n\nTable 2: Experimental results on the home-credit dataset.\
    \ We compare synthetic data produced by GEM with \u03F5 \u2208 \xB6 2 \u21AA 4\
    \ \u2666 against the post-processed synthetic data that are generated by applying\
    \ our post-processing technique with \u03F5$_{post =1 to synthetic data generated\
    \ from GEM with \u03F5 \u2208 \xB6 1 \u21AA 3 \u2666. For reference, the downstream\
    \ model trained and tested on real data has an F1 score of 0 \xB1 24.\n\n| DP\
    \ Mechanism\_\_| Utility Improv.\_\_|\_\_F1 score w/o post-proc. | F1 score w/\
    \ post-proc.\_\_|\_\_JS distance w/o post-proc. | JS distance w/ post-proc.\_\_\
    |\_\_Inverse KL w/o post-proc. | Inverse KL w/ post-proc.\_\_|\n|----------------|-------------------|---------------------------|--------------------------|------------------------------|-----------------------------|-----------------------------|----------------------------|\n\
    | GEM \u03F5 =2\_\_\_\_| 0.57 \xB7 0 \xB1 03\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.19\
    \ | 0.19\xB7 0 \xB1 02\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.93 | 0.97\xB7 0 \xB1 01\_\_\_\_\_|\n\
    | GEM \u03F5 =4\_\_\_\_| 0.68 \xB7 0 \xB1 01\_\_\_|\_\_\_\_\_\_\_\_\_\_\_0.21\
    \ | 0.21\xB7 0 \xB1 01\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_\_0.01 | 0.01\xB7 0 \xB1\
    \ 0\_\_\_\_\_\_|\_\_\_\_\_\_\_\_\_\_\_\_0.95 | 0.99\xB7 0 \xB1 01\_\_\_\_\_|\n\
    \nResult. We present the experimental results on the UCI datasets in Table 1 (and\
    \ Table 3 in Appendix C with a higher privacy budget). We report utility improvement,\
    \ F1 score, and statistical measures. As shown, our post-processing technique\
    \ consistently enhances the utility of synthetic data on selected metrics across\
    \ all benchmark datasets and all privacy mechanisms. Moreover, the utility improvements\
    \ are achieved without degrading the performance of downstream models or statistical\
    \ properties of synthetic data. In other words, our post-processing procedure\
    \ ensures that the logistic regression classifier trained on synthetic data can\
    \ achieve comparable or even higher performance on real test data, while simultaneously\
    \ reducing or maintaining the statistical divergences between synthetic and real\
    \ data.\n\nWe present the experimental results on the home-credit dataset in Table\
    \ 2 and illustrate the misalignment of the correlation matrix with and without\
    \ applying our post-processing procedure in Figure 1. The results demonstrate\
    \ that our algorithm consistently reduces the overall correlation misalignment.\
    \ Additionally, our procedure, which includes computing the utility measures from\
    \ real data, denoising the noisy answers, and computing optimal resampling weights,\
    \ only takes around 4 mins on 1x NVIDIA GeForce RTX 3090 GPU.\n"
  question: 'Is the utility improvement or the difference between no and post-processing
    for the f1 score higher?

    '
task_description: 'answering questions about a provided technical paper

  '
